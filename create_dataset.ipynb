{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Sample_PFF_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should Shotgun and Pistol be NaN for run plays and 0 for passes where no shotgun? Or just zero fine?\n",
    "dataset.SHOTGUN.where(dataset.SHOTGUN.isna(),1, inplace=True)\n",
    "dataset.SHOTGUN.fillna(0, inplace=True)\n",
    "\n",
    "dataset.PISTOL.where(dataset.PISTOL.isna(),1, inplace=True)\n",
    "dataset.PISTOL.fillna(0, inplace=True)\n",
    "\n",
    "dataset['MOFO_PLAYED'] = dataset.MOFOCPLAYED.replace({'O': 1, 'C': 0})\n",
    "dataset['MOFO_SHOWN'] = dataset.MOFOCPLAYED.replace({'O': 1, 'C': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string time (2:00) into seconds as int (120)\n",
    "def convert_time(time_str):\n",
    "    return int(time_str[0:2])*60 + int(time_str[3:])\n",
    "# remove * and +Q to simplify features\n",
    "# decided to group all 3-RB sets together since had similar Run vs Pass rates (and all had low sample sizes)\n",
    "def convert_off_personnel(personnel_str):\n",
    "    new_alignment = personnel_str[0:2]\n",
    "    if new_alignment[0] == '3':\n",
    "        new_alignment = '3+'\n",
    "    if new_alignment == 'Un':\n",
    "        new_alignment = np.nan\n",
    "    return new_alignment\n",
    "# just return nans for values like (10 men, X-X-X)\n",
    "def convert_def_personnel(personnel_str):\n",
    "    try:\n",
    "        num_lineman = int(personnel_str[0])\n",
    "        num_linebackers = int(personnel_str[2])\n",
    "        num_defensivebacks = int(personnel_str[4])\n",
    "    except:\n",
    "        num_lineman = np.nan \n",
    "        num_linebackers = np.nan \n",
    "        num_defensivebacks = np.nan\n",
    "    return pd.Series([num_lineman, num_linebackers, num_defensivebacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CLOCK_INT'] = dataset['CLOCK'].apply(convert_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.QUARTER = dataset.QUARTER.astype(\"category\")\n",
    "dataset.DOWN = dataset.DOWN.astype(\"category\")\n",
    "dataset.OFFTIMEOUTSREMAINING = dataset.OFFTIMEOUTSREMAINING.astype(\"category\")\n",
    "dataset.DEFTIMEOUTSREMAINING = dataset.DEFTIMEOUTSREMAINING.astype(\"category\")\n",
    "dataset.HASH = dataset.HASH.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.loc[(dataset.RUNPASS == 'P') | (dataset.RUNPASS == 'R') ]\n",
    "dataset = dataset.query(\"DOWN != 0\").reset_index(drop=True)\n",
    "dataset.RUNPASS = dataset.RUNPASS.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['OFFPERSONNEL_SIMPLIFIED'] = dataset['OFFPERSONNELBASIC'].apply(convert_off_personnel)\n",
    "dataset[['DEFPERSONNEL_num_linemen', 'DEFPERSONNEL_num_linebackers', 'DEFPERSONNEL_num_defensivebacks']] = dataset['DEFPERSONNEL'].apply(convert_def_personnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc_col_names = [\n",
    "    'historical_yards_per_carry',\n",
    "    'historical_yards_per_pass_attempt', \n",
    "    'historical_yards_allowed_per_carry', \n",
    "    'historical_yards_allowed_per_pass_attempt'\n",
    "]\n",
    "\n",
    "col_names_for_numeric_previous = [\n",
    "'FORCEDFUMBLE',\n",
    "'HIT',\n",
    "'HURRY',\n",
    "'GAINLOSSNET',\n",
    "'INTERCEPTION',\n",
    "'NOHUDDLE',\n",
    "'PENALTY',\n",
    "'PASSDEPTH',\n",
    "'PASSBREAKUP',\n",
    "\n",
    "'DROPBACKDEPTH',\n",
    "\n",
    "'MOFO_PLAYED',\n",
    "'MOFO_SHOWN',\n",
    "'DEFPERSONNEL_num_linemen',\n",
    "'DEFPERSONNEL_num_linebackers',\n",
    "'DEFPERSONNEL_num_defensivebacks',\n",
    "\n",
    "'PISTOL',\n",
    "'PLAYACTION',\n",
    "'SACK',\n",
    "'SCREEN',\n",
    "'SHIFTMOTION',\n",
    "'SHOTGUN',\n",
    "\n",
    "'QBMOVEDOFFSPOT',\n",
    "'QBPRESSURE',\n",
    "'TIMETOPRESSURE',\n",
    "'TIMETOTHROW',\n",
    "'YARDSAFTERCATCH',\n",
    "'YARDSAFTERCONTACT',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all feature names to use in lists, with convention pre_, game_prev_, and historical_prev_\n",
    "prev_names = ['prev_' + x for x in col_names_for_numeric_previous]\n",
    "game_prev_names = ['game_prev_' + x for x in col_names_for_numeric_previous] # instead of just previous, get cumulative mean from all previous plays in game\n",
    "historical_prev_names = ['historical_prev_' + x for x in col_names_for_numeric_previous] # get cumulative mean from all previous plays in all previous games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean_func(g, K=10):\n",
    "    return g.expanding(min_periods=K).mean().shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to sort for adding calculated columns back to original dataframe\n",
    "dataset = dataset.sort_values([\"OffTeam\", \"GAMEID\", \"PLAYID\"]).reset_index(drop=True)\n",
    "dataset[prev_names] = dataset.groupby([\"OffTeam\", \"GAMEID\", \"DRIVE\"], sort=False)[col_names_for_numeric_previous].shift(1)  # don't need to sort since dataset already sorted\n",
    "# shift(1) to exlcude current row from mean calculations\n",
    "dataset[game_prev_names] = dataset.groupby([\"OffTeam\", \"GAMEID\"], sort=False)[col_names_for_numeric_previous].apply(rolling_mean_func).reset_index(drop=True)\n",
    "dataset[historical_prev_names] = dataset.groupby([\"OffTeam\"], sort=False)[col_names_for_numeric_previous].apply(rolling_mean_func, K=100).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['running_gain'] = dataset.RUNPASS.map({\"R\":1, \"P\":np.nan})*dataset.GAINLOSSNET\n",
    "dataset['passing_gain'] = dataset.RUNPASS.map({\"P\":1, \"R\":np.nan})*dataset.GAINLOSSNET\n",
    "dataset['historical_yards_per_carry'] = dataset.groupby([\"OffTeam\"])['running_gain'].apply(rolling_mean_func, K=100).reset_index(drop=True)\n",
    "dataset['historical_yards_per_pass_attempt'] = dataset.groupby([\"OffTeam\"])['passing_gain'].apply(rolling_mean_func, K=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values([\"DefTeam\", \"GAMEID\", \"PLAYID\"]).reset_index(drop=True)\n",
    "dataset['historical_yards_allowed_per_carry'] = dataset.groupby([\"DefTeam\"])['running_gain'].apply(rolling_mean_func, K=100).reset_index(drop=True)\n",
    "dataset['historical_yards_allowed_per_pass_attempt'] = dataset.groupby([\"DefTeam\"])['passing_gain'].apply(rolling_mean_func, K=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_for_categorical_previous = [\n",
    "    'OFFPERSONNEL_SIMPLIFIED', \n",
    "    'CENTERPASSBLOCKDIRECTION'\n",
    "]\n",
    "for col_name in col_names_for_categorical_previous:\n",
    "    dataset[col_name] = dataset[col_name].astype(\"category\")\n",
    "\n",
    "prev_categorical_names = ['prev_' + x for x in col_names_for_categorical_previous]\n",
    "# create previous play categorical\n",
    "dataset[prev_categorical_names] = dataset.groupby([\"OffTeam\",\"GAMEID\", \"DRIVE\"])[col_names_for_categorical_previous].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the names of the columns that are dummy variables for the categorical features used\n",
    "dummy_categorical_names = []\n",
    "for col_name in col_names_for_categorical_previous:\n",
    "    unique_vals = dataset[col_name].unique()\n",
    "    for val in unique_vals:\n",
    "        dummy_column_name = f\"{col_name}_{val}\"\n",
    "        dummy_categorical_names.append(dummy_column_name)\n",
    "game_prev_dummy_categorical_names = ['game_prev_' + x for x in dummy_categorical_names]\n",
    "\n",
    "dummy_dataset = pd.get_dummies(dataset, prefix_sep=\"_\", dummy_na=True, columns=col_names_for_categorical_previous, drop_first=False)\n",
    "dummy_dataset = dummy_dataset.sort_values([\"OffTeam\", \"GAMEID\", \"PLAYID\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset[game_prev_dummy_categorical_names] = dummy_dataset.groupby([\"OffTeam\", \"GAMEID\"])[dummy_categorical_names].apply(rolling_mean_func, K=10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_prev_dummy_categorical_names = ['historical_prev_' + x for x in dummy_categorical_names]\n",
    "dummy_dataset[historical_prev_dummy_categorical_names] = dummy_dataset.groupby([\"OffTeam\"])[dummy_categorical_names].apply(rolling_mean_func, K=100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_names = [\n",
    "    'WEEK',\n",
    "    'QUARTER',\n",
    "    'SCOREDIFFERENTIAL',\n",
    "    'SCORE',\n",
    "    'DISTANCE',\n",
    "    'DOWN',\n",
    "    'FIELDPOSITION',\n",
    "    'DRIVE',\n",
    "    'DRIVEPLAY',\n",
    "    'OFFTIMEOUTSREMAINING',\n",
    "    'DEFTIMEOUTSREMAINING',\n",
    "    'HASH',\n",
    "    'SPOTLEFT',\n",
    "    '2MINUTE', \n",
    "    'CLOCK_INT']\n",
    "engineered_features_names = prev_names + game_prev_names + historical_prev_names + \\\n",
    "                            prev_categorical_names + game_prev_dummy_categorical_names + historical_prev_dummy_categorical_names + \\\n",
    "                            misc_col_names\n",
    "target_names = ['RUNPASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = dummy_dataset.sort_values([\"GAMEID\", \"PLAYID\"]).reset_index(drop=True)    # so aligns with indices used in model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = dummy_dataset[['GAMEID', 'PLAYID'] + base_feature_names + target_names]\n",
    "#base_dataset.to_pickle(\"datasets/base_dataset.pkl\")   # use pickle to keep data dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_categorical_dataset = dummy_dataset[['GAMEID', 'PLAYID'] + base_feature_names + prev_names + game_prev_names + historical_prev_names + \\\n",
    "                                  misc_col_names + target_names]\n",
    "#non_categorical_dataset.to_pickle(\"datasets/non_categorical_dataset.pkl\")   # use pickle to keep data dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset_filtered = dummy_dataset[['GAMEID', 'PLAYID'] + base_feature_names + engineered_features_names + target_names]\n",
    "#dummy_dataset_filtered.to_pickle(\"datasets/dummy_dataset.pkl\")   # use pickle to keep data dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Tests for filtering \n",
    "'''\n",
    "test_df = dummy_dataset.copy(deep=True)\n",
    "test_df = test_df.sort_values([\"OffTeam\", \"GAMEID\", \"PLAYID\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests that we are calculating the rolling means correctly for a given team in a given game\n",
    "# ASSUMES df is sorted how it should be sorted\n",
    "def test_game_prev_calculation(df, game_id, off_team, col_name = 'HURRY'):\n",
    "    slice = df.query(\"GAMEID == @game_id and OffTeam == @off_team\")\n",
    "\n",
    "    #### manually calculate some means #####\n",
    "    hurry_first_10_play_avg = slice[col_name][0:10].mean()\n",
    "    hurry_first_11_play_avg = slice[col_name][0:11].mean()\n",
    "    hurry_first_12_play_avg = slice[col_name][0:12].mean()\n",
    "\n",
    "    assert np.isnan(slice[f'game_prev_{col_name}'].iloc[0])\n",
    "    assert np.isnan(slice[f'game_prev_{col_name}'].iloc[9])\n",
    "    assert hurry_first_10_play_avg == slice[f'game_prev_{col_name}'].iloc[10]\n",
    "    assert hurry_first_11_play_avg == slice[f'game_prev_{col_name}'].iloc[11]\n",
    "    assert hurry_first_12_play_avg == slice[f'game_prev_{col_name}'].iloc[12]\n",
    "    assert slice[col_name][0:-1].mean() == slice[f'game_prev_{col_name}'].iloc[-1]\n",
    "    print(f\"Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n"
     ]
    }
   ],
   "source": [
    "test_game_prev_calculation(df=test_df, game_id=19752, off_team='Team_25')\n",
    "test_game_prev_calculation(df=test_df, game_id=19752, off_team='Team_25', col_name='GAINLOSSNET')\n",
    "test_game_prev_calculation(df=test_df, game_id=19752, off_team='Team_25', col_name='HIT')\n",
    "test_game_prev_calculation(df=test_df, game_id=19746, off_team='Team_28')\n",
    "test_game_prev_calculation(df=test_df, game_id=19663, off_team='Team_22')\n",
    "# check categorical features\n",
    "test_game_prev_calculation(df=test_df, game_id=18548, off_team='Team_3', col_name='CENTERPASSBLOCKDIRECTION_C')\n",
    "test_game_prev_calculation(df=test_df, game_id=18548, off_team='Team_3', col_name='CENTERPASSBLOCKDIRECTION_L')\n",
    "test_game_prev_calculation(df=test_df, game_id=18548, off_team='Team_3', col_name='CENTERPASSBLOCKDIRECTION_R')\n",
    "test_game_prev_calculation(df=test_df, game_id=18548, off_team='Team_3', col_name='CENTERPASSBLOCKDIRECTION_nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests that we are calculating the rolling means correctly for a given team\n",
    "# ASSUMES df is sorted how it should be sorted\n",
    "def test_historical_prev_calculation(df, off_team, col_name = 'CENTERPASSBLOCKDIRECTION_C'):\n",
    "    slice = df.query(\"OffTeam == @off_team\")\n",
    "    max_index = slice.shape[0]\n",
    "\n",
    "    assert np.isnan(slice[f'historical_prev_{col_name}'].iloc[0])\n",
    "    assert np.isnan(slice[f'historical_prev_{col_name}'].iloc[50])\n",
    "    assert np.isnan(slice[f'historical_prev_{col_name}'].iloc[100-1])\n",
    "    assert slice[col_name][0:100].mean() == slice[f'historical_prev_{col_name}'].iloc[100]\n",
    "    assert slice[col_name][0:(max_index//2)].mean()  == slice[f'historical_prev_{col_name}'].iloc[(max_index//2)]\n",
    "    assert slice[col_name][0:-1].mean() == slice[f'historical_prev_{col_name}'].iloc[-1]\n",
    "    print(f\"Successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n",
      "Successful\n",
      "Successful\n",
      "Successful\n"
     ]
    }
   ],
   "source": [
    "test_historical_prev_calculation(df=test_df, off_team='Team_21', col_name='CENTERPASSBLOCKDIRECTION_C')\n",
    "test_historical_prev_calculation(df=test_df, off_team='Team_21', col_name='CENTERPASSBLOCKDIRECTION_L')\n",
    "test_historical_prev_calculation(df=test_df, off_team='Team_21', col_name='CENTERPASSBLOCKDIRECTION_R')\n",
    "test_historical_prev_calculation(df=test_df, off_team='Team_21', col_name='CENTERPASSBLOCKDIRECTION_nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titans_run_pass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
